#+TITLE: Proximal Policy Optimisation (PPO)

https://arxiv.org/pdf/1707.06347.pdf


[[file:./images/code.svg]]

#+BEGIN_SRC python :noweb yes :tangle yes
import pytorch
import gym

<<policy>>

<<value>>

def main(args):

    <<initialise>>

    for k in range(100):
        <<collect-trajectories>>

        <<compute-rewards>>

        <<compute-advantage-estimates>>

        <<update-policy-via-ppo>>

        <<fit-value-function>>
        ...

if __name__ == "__main__":
    args = None
    <<parse-arguments>>

    main(args)

#+END_SRC

* Agent
** Actor (Policy)
:PROPERTIES:
:header-args: :noweb-ref policy
:END:

#+BEGIN_SRC python

#+END_SRC

** Critic (Value)
:PROPERTIES:
:header-args: :noweb-ref value
:END:

* Collect Trajectories
:PROPERTIES:
:header-args: :noweb-ref collect-trajectories
:END:

Run the policy on the [[https://gym.openai.com/envs/BipedalWalker-v2/][BipedalWalker-v2]] environment and save what happens.
#+BEGIN_SRC python
trajectories = []
for trajectory in range(samples):
    done = False
    obs = environment.reset()
    while not done:
        # select action from the policy
        done = true
    # trajectories.append(obs)

#+END_SRC
